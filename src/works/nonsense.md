---
layout: layouts/works/work_nonsense.njk
tags: post
related:
  - Figma
  - UI/UX
  - Product
  - Branding
cover: ./src/img/nonsense_cover.png
subtitle: >
  Product Design Project @ Nonsense
title: >
  Language learning through Movies
date: 2023-01-01
summary_title1: My Role
summary1: Product Design, User journey, Learning journey design, Prototyping
result_title: Timeline
result_content: 2021-2022
summary_title2: Platform
summary2: iOS / Android
link1: nonsense.com
link1_url: https://www.nonsense.com

context: I joined as a founding designer at Nonsense where I led and built core UX and interactions to support a revolutionary edtech app that enables users to learn languages through movies.
section1_title: >
  How to boost language learning with active movie-watching?
section1_subtitle: The Challenge
section1_content_title:
section1_content: Watching a movie is a different experience than learning languages. We needed to ensure that users are actively immersed to better facilitate language learning.{.text-center .fs-5}
section1_title2: >
  How to reflect true learning progress?
section1_content2: "Competitior apps such as Duolingo use XP and day streak which is indirectly tied to true learning progress.  \n asdasdasd {.fs-5}"
section1_title3: >
  How to optimize short attention span?
section1_content3: "Mobile users have short attention span and a language learning app needs to be designed such that it makes the most use of the user's attention span. \n asdasdasd {.fs-5}"
section1_img: ./src/img/diff_image_nonsense.png
section1_img2: ./src/img/sub_challenge_1_nonsense.png
section2_content: I am building an app to support XR learning experiences. I present an approach to map real-world context for multi-modal learning using ChatGPT, SketchFab API and other ML agents to support curiosity and improve knowledge recall. The prototypes allow users to learn languages, science, history, general knowledge and mathematics concepts through the objects around them and the environment.
section2_img: ./src/img/things_overview.png
section2_img2: ./src/img/things_overview2.png
section2_subtitle: Design goals
section2_title: Transform movie watching into a language learning journey
section6_subtitle2: >
  Design Goals
section6_subtitle2_content: Transforming movie watching into a language learning journey
section6_content: The concept of turning sketches into objects such as a chair, car, etc. is interesting and can be extended to drawing objects which don’t exist like purple apples or magical worlds of mushrooms with abstract gradients as the background. The interaction techniques for this expression in 3D spaces could be through 2D sketches and could provide learners with a natural way to imagine and create 3D spaces/objects. This technique could support curiosity by not limiting the learner’s imagination to the paper.
section7_title: Storyboarding
section7_img: ./src/img/p2_pain_points.png
section7_img_caption: User sketch or speech is used to show 3D models from database
section7_img2: ./src/img/viz_support2.png
section7_img2_caption: User sketch is used to show 3D models rendered real-time
section7_content: This prototype aims to understand the benefits and scope of a Mixed Reality visualization support system. To map out the use case of recognizing user intents such as drawings or speech and augmenting them in the MR space, I drew some use case sketches to visualize this tool.
section8_title: Outcomes
section8_img: ./src/img/p2_visualization.png
section8_img_caption: Saying "Can I see mars?" shows 3D model of mars
section8_gif2: ./src/img/apple_viz.gif
section8_img2_caption: Sketching apple renders 3D model of an apple
section8_content: The demo is built on Microsoft Hololens 2, with real-time object detection from the sketch using a custom-trained model. The model is trained on the Google Quickdraw dataset and works with around 350 common objects (chairs, cars, apples, etc.). This allows for real-time searching for models such as an “apple”, filtering and finding a suitable model that is supported by Hololens 2, downloading it, and then rendering it in front of the user. The user can then use their hands to interact with these objects to either scale, rotate or move them in the space.

section8_subtitle2: >
  "What if I want to know something about an apple?"
section8a_title: Storyboarding
section8_subtitle2_content: Help them learn anything they want to learn
section8a_img: ./src/img/viz_arch.png
section8a_content: After brainstorming and sketching ideas, some of the use case scenarios could be the following. The user could ask questions about the model, parts of a model or one of the many models loaded. I used Wit.ai to recognize user intent and ChatGPT API to support learners with as many questions as they had while exploring these models.

section8b_title: Outcomes
section8b_img: ./src/img/cxr_screenshots.png
section8b_content: >
  The application is being built and tested for the Quest Platform and can bring learning content real-time in the form of text, images, videos, 3D models to provide engaging learning experiences. The application also uses Wit.ai to understand user intent and collect the relevant information required to provide the best learning experience to the user.

section9_title: Work in progress..
section9_subtitle: Building tools to support limitless learning
section9_img: ./src/img/final_grid_things.png
section9_content: This research process and user study is currently in progress and has helped in the development of CuriosityXR, resulting in a system that provides engaging and immersive learning experiences that support learners' curiosity. The application is planned to launch in April 2023 on Quest store, Register for the waitlist on the link below.
---

## I am some content
